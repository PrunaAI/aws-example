{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3019e08e",
   "metadata": {},
   "source": [
    "# Triton on SageMaker with Pruna\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc7961c",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "\n",
    "Installs the dependencies required to package the model and run inferences using Triton server.\n",
    "\n",
    "Also define the IAM role that will give SageMaker access to the model artifacts and the NVIDIA Triton ECR image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb5ae6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pruna 0.2.4 requires opentelemetry-api>=1.30.0, but you have opentelemetry-api 1.26.0 which is incompatible.\n",
      "pruna 0.2.4 requires opentelemetry-exporter-otlp>=1.29.0, but you have opentelemetry-exporter-otlp 1.26.0 which is incompatible.\n",
      "pruna 0.2.4 requires opentelemetry-sdk>=1.30.0, but you have opentelemetry-sdk 1.26.0 which is incompatible.\n",
      "pruna 0.2.4 requires torch==2.7.0, but you have torch 2.6.0 which is incompatible.\n",
      "pruna 0.2.4 requires torchvision==0.22.0, but you have torchvision 0.21.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: nvidia-pyindex in /home/ubuntu/miniconda3/envs/vllm/lib/python3.10/site-packages (1.0.9)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tritonclient[http] in /home/ubuntu/miniconda3/envs/vllm/lib/python3.10/site-packages (2.58.0)\n",
      "Requirement already satisfied: numpy>=1.19.1 in /home/ubuntu/miniconda3/envs/vllm/lib/python3.10/site-packages (from tritonclient[http]) (1.26.4)\n",
      "Requirement already satisfied: python-rapidjson>=0.9.1 in /home/ubuntu/miniconda3/envs/vllm/lib/python3.10/site-packages (from tritonclient[http]) (1.20)\n",
      "Requirement already satisfied: urllib3>=2.0.7 in /home/ubuntu/miniconda3/envs/vllm/lib/python3.10/site-packages (from tritonclient[http]) (2.4.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.1 in /home/ubuntu/miniconda3/envs/vllm/lib/python3.10/site-packages (from tritonclient[http]) (3.11.18)\n",
      "Requirement already satisfied: geventhttpclient>=2.3.3 in /home/ubuntu/miniconda3/envs/vllm/lib/python3.10/site-packages (from tritonclient[http]) (2.3.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ubuntu/miniconda3/envs/vllm/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[http]) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/miniconda3/envs/vllm/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[http]) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ubuntu/miniconda3/envs/vllm/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[http]) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/miniconda3/envs/vllm/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[http]) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/miniconda3/envs/vllm/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[http]) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/miniconda3/envs/vllm/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[http]) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ubuntu/miniconda3/envs/vllm/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[http]) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ubuntu/miniconda3/envs/vllm/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.1->tritonclient[http]) (1.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/ubuntu/miniconda3/envs/vllm/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp<4.0.0,>=3.8.1->tritonclient[http]) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.0 in /home/ubuntu/miniconda3/envs/vllm/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp<4.0.0,>=3.8.1->tritonclient[http]) (3.10)\n",
      "Requirement already satisfied: gevent in /home/ubuntu/miniconda3/envs/vllm/lib/python3.10/site-packages (from geventhttpclient>=2.3.3->tritonclient[http]) (25.5.1)\n",
      "Requirement already satisfied: certifi in /home/ubuntu/miniconda3/envs/vllm/lib/python3.10/site-packages (from geventhttpclient>=2.3.3->tritonclient[http]) (2025.4.26)\n",
      "Requirement already satisfied: brotli in /home/ubuntu/miniconda3/envs/vllm/lib/python3.10/site-packages (from geventhttpclient>=2.3.3->tritonclient[http]) (1.1.0)\n",
      "Requirement already satisfied: greenlet>=3.2.2 in /home/ubuntu/miniconda3/envs/vllm/lib/python3.10/site-packages (from gevent->geventhttpclient>=2.3.3->tritonclient[http]) (3.2.2)\n",
      "Requirement already satisfied: zope.event in /home/ubuntu/miniconda3/envs/vllm/lib/python3.10/site-packages (from gevent->geventhttpclient>=2.3.3->tritonclient[http]) (5.0)\n",
      "Requirement already satisfied: zope.interface in /home/ubuntu/miniconda3/envs/vllm/lib/python3.10/site-packages (from gevent->geventhttpclient>=2.3.3->tritonclient[http]) (7.2)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/miniconda3/envs/vllm/lib/python3.10/site-packages (from zope.event->gevent->geventhttpclient>=2.3.3->tritonclient[http]) (78.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU pip awscli boto3 sagemaker transformers\n",
    "!pip install nvidia-pyindex\n",
    "!pip install tritonclient[http]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d068e6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ['AWS_DEFAULT_REGION'] = \"us-east-1\"\n",
    "#os.environ['AWS_ACCESS_KEY_ID'] = \"\"\n",
    "#os.environ['AWS_SECRET_ACCESS_KEY'] = \"\"\n",
    "#os.environ['AWS_SESSION_TOKEN'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ba8bbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ubuntu/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3, json, sagemaker, time\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = boto3.Session()\n",
    "\n",
    "sm = sess.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess, sagemaker_client=sm)\n",
    "role = get_execution_role()\n",
    "client = boto3.client(\"sagemaker-runtime\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "393af1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id_map = {\n",
    "    'us-east-1': '785573368785',\n",
    "    'us-east-2': '007439368137',\n",
    "    'us-west-1': '710691900526',\n",
    "    'us-west-2': '301217895009',\n",
    "    'eu-west-1': '802834080501',\n",
    "    'eu-west-2': '205493899709',\n",
    "    'eu-west-3': '254080097072',\n",
    "    'eu-north-1': '601324751636',\n",
    "    'eu-south-1': '966458181534',\n",
    "    'eu-central-1': '746233611703',\n",
    "    'ap-east-1': '110948597952',\n",
    "    'ap-south-1': '763008648453',\n",
    "    'ap-northeast-1': '941853720454',\n",
    "    'ap-northeast-2': '151534178276',\n",
    "    'ap-southeast-1': '324986816169',\n",
    "    'ap-southeast-2': '355873309152',\n",
    "    'cn-northwest-1': '474822919863',\n",
    "    'cn-north-1': '472730292857',\n",
    "    'sa-east-1': '756306329178',\n",
    "    'ca-central-1': '464438896020',\n",
    "    'me-south-1': '836785723513',\n",
    "    'af-south-1': '774647643957'\n",
    "}\n",
    "account_id_map['us-east-1'] = \"763104351884\"\n",
    "account_id_map['eu-west-1'] = \"763104351884\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e88b9be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "if region not in account_id_map.keys():\n",
    "    raise(\"UNSUPPORTED REGION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e951400",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "triton_image_uri = \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:24.09-py3\".format(\n",
    "    account_id=account_id_map[region], region=region, base=base\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce5f7a7",
   "metadata": {},
   "source": [
    "## PyTorch NLP-Llama-Instruct\n",
    "\n",
    "For a simple use case we will take the pre-trained NLP llama-8B model from [Hugging Face](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct), compress it with `pruna` and deploy it on SageMaker with Triton as the model server. We used the pre-configured `config.pbtxt` file provided with this repo [here](./triton-serve-pt/bert/config.pbtxt) to specify model [configuration](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md) which Triton uses to load the model. We tar the model directory and upload it to s3 to later create a [SageMaker Model](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html).\n",
    "\n",
    "**Note**: SageMaker expects the model tarball file to have a top level directory with the same name as the model defined in the `config.pbtxt`.\n",
    "\n",
    "```\n",
    "llama\n",
    "├── 1\n",
    "│   └── model.py\n",
    "└── config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5623e0f7",
   "metadata": {},
   "source": [
    "### PyTorch: Packaging model files and uploading to s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cc651c",
   "metadata": {},
   "source": [
    "Copy model into triton structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fe7ed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p triton-serve-pt/llama/1/\n",
    "!cp workspace/model.py triton-serve-pt/llama/1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3125cfd6",
   "metadata": {},
   "source": [
    "Create dedicated conda env, and move it to triton structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb9d1e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.1.1\n",
      "    latest version: 25.5.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ubuntu/miniconda3/envs/hf_env\n",
      "\n",
      "  added / updated specs:\n",
      "    - python=3.10\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main \n",
      "  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu \n",
      "  bzip2              pkgs/main/linux-64::bzip2-1.0.8-h5eee18b_6 \n",
      "  ca-certificates    pkgs/main/linux-64::ca-certificates-2025.2.25-h06a4308_0 \n",
      "  expat              pkgs/main/linux-64::expat-2.7.1-h6a678d5_0 \n",
      "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.40-h12ee557_0 \n",
      "  libffi             pkgs/main/linux-64::libffi-3.4.4-h6a678d5_1 \n",
      "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-11.2.0-h1234567_1 \n",
      "  libgomp            pkgs/main/linux-64::libgomp-11.2.0-h1234567_1 \n",
      "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-11.2.0-h1234567_1 \n",
      "  libuuid            pkgs/main/linux-64::libuuid-1.41.5-h5eee18b_0 \n",
      "  libxcb             pkgs/main/linux-64::libxcb-1.17.0-h9b100fa_0 \n",
      "  ncurses            pkgs/main/linux-64::ncurses-6.4-h6a678d5_0 \n",
      "  openssl            pkgs/main/linux-64::openssl-3.0.16-h5eee18b_0 \n",
      "  pip                pkgs/main/noarch::pip-25.1-pyhc872135_2 \n",
      "  pthread-stubs      pkgs/main/linux-64::pthread-stubs-0.3-h0ce48e5_1 \n",
      "  python             pkgs/main/linux-64::python-3.10.18-h1a3bd86_0 \n",
      "  readline           pkgs/main/linux-64::readline-8.2-h5eee18b_0 \n",
      "  setuptools         pkgs/main/linux-64::setuptools-78.1.1-py310h06a4308_0 \n",
      "  sqlite             pkgs/main/linux-64::sqlite-3.45.3-h5eee18b_0 \n",
      "  tk                 pkgs/main/linux-64::tk-8.6.14-h993c535_1 \n",
      "  tzdata             pkgs/main/noarch::tzdata-2025b-h04d1e81_0 \n",
      "  wheel              pkgs/main/linux-64::wheel-0.45.1-py310h06a4308_0 \n",
      "  xorg-libx11        pkgs/main/linux-64::xorg-libx11-1.8.12-h9b100fa_1 \n",
      "  xorg-libxau        pkgs/main/linux-64::xorg-libxau-1.0.12-h9b100fa_0 \n",
      "  xorg-libxdmcp      pkgs/main/linux-64::xorg-libxdmcp-1.1.5-h9b100fa_0 \n",
      "  xorg-xorgproto     pkgs/main/linux-64::xorg-xorgproto-2024.1-h5eee18b_1 \n",
      "  xz                 pkgs/main/linux-64::xz-5.6.4-h5eee18b_1 \n",
      "  zlib               pkgs/main/linux-64::zlib-1.2.13-h5eee18b_1 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate hf_env\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.1.1\n",
      "    latest version: 25.5.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "^C\n",
      "\n",
      "CondaSystemExit: \n",
      "Operation aborted.  Exiting.\n",
      "\n",
      "mv: cannot stat 'hf_env.tar.gz': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!bash workspace/create_hf_env.sh\n",
    "!mv hf_env.tar.gz triton-serve-pt/llama/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd5852",
   "metadata": {},
   "source": [
    "Locally save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bca1b7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09f507cfa214d94864d7b8f7825b32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187b0d004e144531b00ce56053dda31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c34dfcbd9e74b74a946ed97d8aadf69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f6b7acf3fb41259b3ec29d5160dda7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1ab71214e14451a4d370751b75a471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359915f6ac814a6695703797f86f13c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/186 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from pathlib import Path\n",
    "\n",
    "model_id = \"NousResearch/Llama-3.2-1B\"\n",
    "output_dir = Path(\"triton-serve-pt/llama/1\")  # points to the version folder\n",
    "\n",
    "# Download and save locally\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "tokenizer.save_pretrained(output_dir / \"tokenizer\")\n",
    "model.save_pretrained(output_dir / \"model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36a4bc1",
   "metadata": {},
   "source": [
    "Package the model and env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aaa9d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -C triton-serve-pt/ -czf llama.tar.gz llama\n",
    "model_uri = sagemaker_session.upload_data(path=\"llama.tar.gz\", key_prefix=\"triton-serve-pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080e5e0b",
   "metadata": {},
   "source": [
    "### PyTorch: Create SageMaker Endpoint\n",
    "\n",
    "We start off by creating a [sagemaker model](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateModel.html) from the model files we uploaded to s3 in the previous step.\n",
    "\n",
    "In this step we also provide an additional Environment Variable i.e. `SAGEMAKER_TRITON_DEFAULT_MODEL_NAME` which specifies the name of the model to be loaded by Triton. **The value of this key should match the folder name in the model package uploaded to s3**. This variable is optional in case of a single model. In case of ensemble models, this key **has to be** specified for Triton to startup in SageMaker.\n",
    "\n",
    "Additionally, customers can set `SAGEMAKER_TRITON_BUFFER_MANAGER_THREAD_COUNT` and `SAGEMAKER_TRITON_THREAD_COUNT` for optimizing the thread counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68685ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763104351884.dkr.ecr.us-east-1.amazonaws.com/sagemaker-tritonserver:24.09-py3\n",
      "s3://sagemaker-us-east-1-992382637587/triton-serve-pt/llama.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(triton_image_uri)\n",
    "print(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfcd015d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Arn: arn:aws:sagemaker:us-east-1:992382637587:model/triton-nlp-llama-pt-2025-06-11-12-38-59\n"
     ]
    }
   ],
   "source": [
    "sm_model_name = \"triton-nlp-llama-pt-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "container = {\n",
    "    \"Image\": triton_image_uri,\n",
    "    \"ModelDataUrl\": model_uri,\n",
    "    \"Environment\": {\"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME\": \"llama\"},\n",
    "}\n",
    "\n",
    "create_model_response = sm.create_model(\n",
    "    ModelName=sm_model_name, ExecutionRoleArn=\"arn:aws:iam::992382637587:role/sharedservices-sagemaker-role\", PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09337c37",
   "metadata": {},
   "source": [
    "Using the model above, we create an [endpoint configuration](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateEndpointConfig.html) where we can specify the type and number of instances we want in the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54090bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Config Arn: arn:aws:sagemaker:us-east-1:992382637587:endpoint-config/triton-nlp-llama-pt-2025-06-11-12-39-02\n"
     ]
    }
   ],
   "source": [
    "endpoint_config_name = \"triton-nlp-llama-pt-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_config_response = sm.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g4dn.2xlarge\", #\"ml.g6e.2xlarge\",\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f524cb7",
   "metadata": {},
   "source": [
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to **InService** once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "add88402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Arn: arn:aws:sagemaker:us-east-1:992382637587:endpoint/triton-nlp-llama-pt-2025-06-11-12-50-50\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = \"triton-nlp-llama-pt-\" + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "create_endpoint_response = sm.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a85aeba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring endpoint: triton-nlp-llama-pt-2025-06-11-12-50-50 in region us-east-1\n",
      "Log group: /aws/sagemaker/Endpoints/triton-nlp-llama-pt-2025-06-11-12-50-50\n",
      "Polling every 30 seconds.\n",
      "\n",
      "\n",
      "--- 2025-06-11 12:51:18 UTC ---\n",
      "Endpoint Status: Creating\n",
      "  Log group /aws/sagemaker/Endpoints/triton-nlp-llama-pt-2025-06-11-12-50-50 not found yet. This is normal during initial endpoint creation.\n",
      "\n",
      "--- 2025-06-11 12:51:48 UTC ---\n",
      "Endpoint Status: Creating\n",
      "  Log group /aws/sagemaker/Endpoints/triton-nlp-llama-pt-2025-06-11-12-50-50 not found yet. This is normal during initial endpoint creation.\n",
      "\n",
      "--- 2025-06-11 12:52:19 UTC ---\n",
      "Endpoint Status: Creating\n",
      "  Log group /aws/sagemaker/Endpoints/triton-nlp-llama-pt-2025-06-11-12-50-50 not found yet. This is normal during initial endpoint creation.\n",
      "\n",
      "--- 2025-06-11 12:52:49 UTC ---\n",
      "Endpoint Status: Creating\n",
      "  Log group /aws/sagemaker/Endpoints/triton-nlp-llama-pt-2025-06-11-12-50-50 not found yet. This is normal during initial endpoint creation.\n",
      "\n",
      "--- 2025-06-11 12:53:19 UTC ---\n",
      "Endpoint Status: Creating\n",
      "  Log group /aws/sagemaker/Endpoints/triton-nlp-llama-pt-2025-06-11-12-50-50 not found yet. This is normal during initial endpoint creation.\n",
      "\n",
      "--- 2025-06-11 12:53:49 UTC ---\n",
      "Endpoint Status: Creating\n",
      "  Log group /aws/sagemaker/Endpoints/triton-nlp-llama-pt-2025-06-11-12-50-50 not found yet. This is normal during initial endpoint creation.\n",
      "\n",
      "--- 2025-06-11 12:54:19 UTC ---\n",
      "Endpoint Status: Creating\n",
      "  Log group /aws/sagemaker/Endpoints/triton-nlp-llama-pt-2025-06-11-12-50-50 not found yet. This is normal during initial endpoint creation.\n",
      "\n",
      "--- 2025-06-11 12:54:49 UTC ---\n",
      "Endpoint Status: Creating\n",
      "  Log group /aws/sagemaker/Endpoints/triton-nlp-llama-pt-2025-06-11-12-50-50 not found yet. This is normal during initial endpoint creation.\n",
      "\n",
      "--- 2025-06-11 12:55:19 UTC ---\n",
      "Endpoint Status: Creating\n",
      "  Log group /aws/sagemaker/Endpoints/triton-nlp-llama-pt-2025-06-11-12-50-50 not found yet. This is normal during initial endpoint creation.\n",
      "\n",
      "--- 2025-06-11 12:55:50 UTC ---\n",
      "Endpoint Status: Creating\n",
      "  Log group /aws/sagemaker/Endpoints/triton-nlp-llama-pt-2025-06-11-12-50-50 not found yet. This is normal during initial endpoint creation.\n",
      "\n",
      "--- 2025-06-11 12:56:20 UTC ---\n",
      "Endpoint Status: Creating\n",
      "  Logs from stream: AllTraffic/i-0309596646a813b25\n",
      "    2025-06-11 12:55:57 UTC: =============================\n",
      "    2025-06-11 12:55:57 UTC: == Triton Inference Server ==\n",
      "    2025-06-11 12:55:57 UTC: =============================\n",
      "    2025-06-11 12:55:57 UTC: NVIDIA Release 24.09 (build <unknown>)\n",
      "    2025-06-11 12:55:57 UTC: Triton Server Version 2.50.0\n",
      "    2025-06-11 12:55:57 UTC: Copyright (c) 2018-2024, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
      "    2025-06-11 12:55:57 UTC: Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
      "    2025-06-11 12:55:57 UTC: This container image and its contents are governed by the NVIDIA Deep Learning Container License.\n",
      "    2025-06-11 12:55:57 UTC: By pulling and using the container, you accept the terms and conditions of this license:\n",
      "    2025-06-11 12:55:57 UTC: https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n",
      "    2025-06-11 12:55:57 UTC: NOTE: CUDA Forward Compatibility mode ENABLED.\n",
      "  Using CUDA 12.6 driver version 560.35.03 with kernel driver version 470.256.02.\n",
      "  See https://docs.nvidia.com/deploy/cuda-compatibility/ for details.\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.738720 122 cache_manager.cc:480] \"Create CacheManager with cache_dir: '/opt/tritonserver/caches'\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.913629 122 pinned_memory_manager.cc:277] \"Pinned memory pool is created at '0x7f42b4000000' with size 268435456\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.913960 122 cuda_memory_manager.cc:107] \"CUDA memory pool is created on device 0 with size 67108864\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.915737 122 model_config_utils.cc:716] \"Server side auto-completed config: \"\n",
      "    2025-06-11 12:55:57 UTC: name: \"llama\"\n",
      "    2025-06-11 12:55:57 UTC: max_batch_size: 4\n",
      "    2025-06-11 12:55:57 UTC: input {\n",
      "  name: \"INPUT_TEXT\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 1\n",
      "    2025-06-11 12:55:57 UTC: }\n",
      "    2025-06-11 12:55:57 UTC: input {\n",
      "  name: \"MAX_TOKENS\"\n",
      "  data_type: TYPE_INT32\n",
      "  dims: 1\n",
      "    2025-06-11 12:55:57 UTC: }\n",
      "    2025-06-11 12:55:57 UTC: output {\n",
      "  name: \"OUTPUT_TEXT\"\n",
      "  data_type: TYPE_STRING\n",
      "  dims: 1\n",
      "    2025-06-11 12:55:57 UTC: }\n",
      "    2025-06-11 12:55:57 UTC: instance_group {\n",
      "  kind: KIND_GPU\n",
      "    2025-06-11 12:55:57 UTC: }\n",
      "    2025-06-11 12:55:57 UTC: default_model_filename: \"model.py\"\n",
      "    2025-06-11 12:55:57 UTC: parameters {\n",
      "  key: \"EXECUTION_ENV_PATH\"\n",
      "  value {\n",
      "    string_value: \"$$TRITON_MODEL_DIRECTORY/hf_env.tar.gz\"\n",
      "  }\n",
      "    2025-06-11 12:55:57 UTC: }\n",
      "    2025-06-11 12:55:57 UTC: backend: \"python\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.915836 122 model_lifecycle.cc:472] \"loading: /opt/ml/model/::llama:1\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.915990 122 backend_model.cc:503] \"Adding default backend config setting: default-max-batch-size,4\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.916033 122 shared_library.cc:112] \"OpenLibraryHandle: /opt/tritonserver/backends/python/libtriton_python.so\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.917387 122 python_be.cc:1618] \"'python' TRITONBACKEND API version: 1.19\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.917404 122 python_be.cc:1640] \"backend configuration:\\n{\\\"cmdline\\\":{\\\"auto-complete-config\\\":\\\"true\\\",\\\"backend-directory\\\":\\\"/opt/tritonserver/backends\\\",\\\"min-compute-capability\\\":\\\"6.000000\\\",\\\"shm-default-byte-size\\\":\\\"16777216\\\",\\\"shm-growth-byte-size\\\":\\\"1048576\\\",\\\"default-max-batch-size\\\":\\\"4\\\"}}\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.917427 122 python_be.cc:1778] \"Shared memory configuration is shm-default-byte-size=16777216,shm-growth-byte-size=1048576,stub-timeout-seconds=30\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.917511 122 python_be.cc:2075] \"TRITONBACKEND_GetBackendAttribute: setting attributes\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.917813 122 python_be.cc:1879] \"TRITONBACKEND_ModelInitialize: llama (version 1)\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.918241 122 model_config_utils.cc:1941] \"ModelConfig 64-bit fields:\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.918255 122 model_config_utils.cc:1943] \"\\tModelConfig::dynamic_batching::default_priority_level\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.918259 122 model_config_utils.cc:1943] \"\\tModelConfig::dynamic_batching::default_queue_policy::default_timeout_microseconds\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.918264 122 model_config_utils.cc:1943] \"\\tModelConfig::dynamic_batching::max_queue_delay_microseconds\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.918271 122 model_config_utils.cc:1943] \"\\tModelConfig::dynamic_batching::priority_levels\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.918277 122 model_config_utils.cc:1943] \"\\tModelConfig::dynamic_batching::priority_queue_policy::key\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.918283 122 model_config_utils.cc:1943] \"\\tModelConfig::dynamic_batching::priority_queue_policy::value::default_timeout_microseconds\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.918294 122 model_config_utils.cc:1943] \"\\tModelConfig::ensemble_scheduling::step::model_version\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.918300 122 model_config_utils.cc:1943] \"\\tModelConfig::input::dims\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.918306 122 model_config_utils.cc:1943] \"\\tModelConfig::input::reshape::shape\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.918317 122 model_config_utils.cc:1943] \"\\tModelConfig::instance_group::secondary_devices::device_id\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.918323 122 model_config_utils.cc:1943] \"\\tModelConfig::model_warmup::inputs::value::dims\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.918333 122 model_config_utils.cc:1943] \"\\tModelConfig::optimization::cuda::graph_spec::graph_lower_bound::input::value::dim\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.918339 122 model_config_utils.cc:1943] \"\\tModelConfig::optimization::cuda::graph_spec::input::value::dim\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.918350 122 model_config_utils.cc:1943] \"\\tModelConfig::output::dims\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.918356 122 model_config_utils.cc:1943] \"\\tModelConfig::output::reshape::shape\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.918367 122 model_config_utils.cc:1943] \"\\tModelConfig::sequence_batching::direct::max_queue_delay_microseconds\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.918373 122 model_config_utils.cc:1943] \"\\tModelConfig::sequence_batching::max_sequence_idle_microseconds\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.918384 122 model_config_utils.cc:1943] \"\\tModelConfig::sequence_batching::oldest::max_queue_delay_microseconds\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.918390 122 model_config_utils.cc:1943] \"\\tModelConfig::sequence_batching::state::dims\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.918396 122 model_config_utils.cc:1943] \"\\tModelConfig::sequence_batching::state::initial_state::dims\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.918403 122 model_config_utils.cc:1943] \"\\tModelConfig::version_policy::specific::versions\"\n",
      "    2025-06-11 12:55:57 UTC: I0611 12:55:56.918553 122 python_be.cc:1485] \"Using Python execution env /opt/ml/model/llama/hf_env.tar.gz\"\n",
      "    2025-06-11 12:56:02 UTC: I0611 12:55:56.918607 122 pb_env.cc:292] \"Extracting Python execution env /opt/ml/model/llama/hf_env.tar.gz\"\n",
      "\n",
      "--- 2025-06-11 12:56:50 UTC ---\n",
      "Endpoint Status: Creating\n",
      "  Logs from stream: AllTraffic/i-0309596646a813b25\n",
      "    2025-06-11 12:56:40 UTC: I0611 12:56:36.216370 122 stub_launcher.cc:385] \"Starting Python backend stub: source /tmp/python_env_cgeuzH/0/bin/activate && exec env LD_LIBRARY_PATH=/tmp/python_env_cgeuzH/0/lib:$LD_LIBRARY_PATH /opt/tritonserver/backends/python/triton_python_backend_stub /opt/ml/model/llama/1/model.py triton_python_backend_shm_region_8a81a145-6b37-4c69-a869-2ee501362cc9 16777216 1048576 122 /opt/tritonserver/backends/python 336 llama DEFAULT\"\n",
      "\n",
      "--- 2025-06-11 12:57:20 UTC ---\n",
      "Endpoint Status: Creating\n",
      "  Logs from stream: AllTraffic/i-0309596646a813b25\n",
      "    2025-06-11 12:56:47 UTC: I0611 12:56:46.934873 122 python_be.cc:1574] \"model configuration:\\n{\\n    \\\"name\\\": \\\"llama\\\",\\n    \\\"platform\\\": \\\"\\\",\\n    \\\"backend\\\": \\\"python\\\",\\n    \\\"runtime\\\": \\\"\\\",\\n    \\\"version_policy\\\": {\\n        \\\"latest\\\": {\\n            \\\"num_versions\\\": 1\\n        }\\n    },\\n    \\\"max_batch_size\\\": 4,\\n    \\\"input\\\": [\\n        {\\n            \\\"name\\\": \\\"INPUT_TEXT\\\",\\n            \\\"data_type\\\": \\\"TYPE_STRING\\\",\\n            \\\"format\\\": \\\"FORMAT_NONE\\\",\\n            \\\"dims\\\": [\\n                1\\n            ],\\n            \\\"is_shape_tensor\\\": false,\\n            \\\"allow_ragged_batch\\\": false,\\n            \\\"optional\\\": false,\\n            \\\"is_non_linear_format_io\\\": false\\n        },\\n        {\\n            \\\"name\\\": \\\"MAX_TOKENS\\\",\\n            \\\"data_type\\\": \\\"TYPE_INT32\\\",\\n            \\\"format\\\": \\\"FORMAT_NONE\\\",\\n            \\\"dims\\\": [\\n                1\\n            ],\\n            \\\"is_shape_tensor\\\": false,\\n            \\\"allow_ragged_batch\\\": false,\\n            \\\"optional\\\": false,\\n            \\\"is_non_linear_format_io\\\": false\\n        }\\n    ],\\n    \\\"output\\\": [\\n        {\\n            \\\"name\\\": \\\"OUTPUT_TEXT\\\",\\n            \\\"data_type\\\": \\\"TYPE_STRING\\\",\\n            \\\"dims\\\": [\\n                1\\n            ],\\n            \\\"label_filename\\\": \\\"\\\",\\n            \\\"is_shape_tensor\\\": false,\\n            \\\"is_non_linear_format_io\\\": false\\n        }\\n    ],\\n    \\\"batch_input\\\": [],\\n    \\\"batch_output\\\": [],\\n    \\\"optimization\\\": {\\n        \\\"priority\\\": \\\"PRIORITY_DEFAULT\\\",\\n        \\\"input_pinned_memory\\\": {\\n            \\\"enable\\\": true\\n        },\\n        \\\"output_pinned_memory\\\": {\\n            \\\"enable\\\": true\\n        },\\n        \\\"gather_kernel_buffer_threshold\\\": 0,\\n        \\\"eager_batching\\\": false\\n    },\\n    \\\"instance_group\\\": [\\n        {\\n            \\\"name\\\": \\\"llama_0\\\",\\n            \\\"kind\\\": \\\"KIND_GPU\\\",\\n            \\\"count\\\": 1,\\n            \\\"gpus\\\": [\\n                0\\n            ],\\n            \\\"secondary_devices\\\": [],\\n            \\\"profile\\\": [],\\n            \\\"passive\\\": false,\\n            \\\"host_policy\\\": \\\"\\\"\\n        }\\n    ],\\n    \\\"default_model_filename\\\": \\\"model.py\\\",\\n    \\\"cc_model_filenames\\\": {},\\n    \\\"metric_tags\\\": {},\\n    \\\"parameters\\\": {\\n        \\\"EXECUTION_ENV_PATH\\\": {\\n            \\\"string_value\\\": \\\"$$TRITON_MODEL_DIRECTORY/hf_env.tar.gz\\\"\\n        }\\n    },\\n    \\\"model_warmup\\\": []\\n}\"\n",
      "    2025-06-11 12:56:47 UTC: I0611 12:56:46.935154 122 python_be.cc:1923] \"TRITONBACKEND_ModelInstanceInitialize: llama_0_0 (GPU device 0)\"\n",
      "    2025-06-11 12:56:47 UTC: I0611 12:56:46.935359 122 backend_model_instance.cc:106] \"Creating instance llama_0_0 on GPU 0 (7.5) using artifact 'model.py'\"\n",
      "    2025-06-11 12:56:51 UTC: I0611 12:56:46.937564 122 stub_launcher.cc:385] \"Starting Python backend stub: source /tmp/python_env_cgeuzH/0/bin/activate && exec env LD_LIBRARY_PATH=/tmp/python_env_cgeuzH/0/lib:$LD_LIBRARY_PATH /opt/tritonserver/backends/python/triton_python_backend_stub /opt/ml/model/llama/1/model.py triton_python_backend_shm_region_04bbe194-6c31-4293-9fd4-456b10d47eff 16777216 1048576 122 /opt/tritonserver/backends/python 336 llama_0_0 DEFAULT\"\n",
      "    2025-06-11 12:57:10 UTC: INFO - No device specified. Using best available device: 'cuda'\n",
      "    2025-06-11 12:57:13 UTC: INFO - Starting quantizer hqq...\n",
      "\n",
      "--- 2025-06-11 12:57:51 UTC ---\n",
      "Endpoint Status: InService\n",
      "Endpoint is InService.\n",
      "\n",
      "--- Monitoring finished ---\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import time\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "endpoint_name = \"\"  # REQUIRED: Replace with your endpoint name\n",
    "region_name = region   # REQUIRED: Replace with your AWS region\n",
    "poll_interval_seconds = 30        # How often to check status and logs\n",
    "\n",
    "# --- Initialization ---\n",
    "# Ensure AWS credentials are configured (e.g., via environment variables, AWS CLI, or IAM role)\n",
    "# If running this snippet outside the notebook where 'sess' was defined,\n",
    "# you'll need to create a new boto3 session and clients.\n",
    "try:\n",
    "    # Attempt to use existing session if available (e.g. if run in same kernel after notebook cells)\n",
    "    # This is a common pattern but might not always be the case depending on execution environment.\n",
    "    if 'sess' not in locals() or sess is None:\n",
    "        print(\"Creating new Boto3 session.\")\n",
    "        sess = boto3.Session(region_name=region_name)\n",
    "except NameError:\n",
    "    print(\"Creating new Boto3 session (sess not defined).\")\n",
    "    sess = boto3.Session(region_name=region_name)\n",
    "\n",
    "sm_client = sess.client(\"sagemaker\")\n",
    "logs_client = sess.client(\"logs\")\n",
    "\n",
    "last_event_timestamps = {}  # To store {stream_name: last_timestamp_processed}\n",
    "log_group_name = f\"/aws/sagemaker/Endpoints/{endpoint_name}\"\n",
    "\n",
    "print(f\"Monitoring endpoint: {endpoint_name} in region {region_name}\")\n",
    "print(f\"Log group: {log_group_name}\")\n",
    "print(f\"Polling every {poll_interval_seconds} seconds.\\n\")\n",
    "\n",
    "# --- Monitoring Loop ---\n",
    "while True:\n",
    "    try:\n",
    "        resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "        status = resp[\"EndpointStatus\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error describing endpoint {endpoint_name}: {e}\")\n",
    "        print(\"Stopping monitoring.\")\n",
    "        break\n",
    "\n",
    "    current_time_utc_str = time.strftime('%Y-%m-%d %H:%M:%S UTC', time.gmtime())\n",
    "    print(f\"\\n--- {current_time_utc_str} ---\")\n",
    "    print(f\"Endpoint Status: {status}\")\n",
    "\n",
    "    if status == \"Failed\":\n",
    "        print(f\"FailureReason: {resp.get('FailureReason', 'N/A')}\")\n",
    "        break\n",
    "    if status == \"InService\":\n",
    "        print(\"Endpoint is InService.\")\n",
    "        break\n",
    "    if status not in [\"Creating\", \"Updating\"]: # Other terminal states\n",
    "        print(f\"Endpoint is in a terminal state: {status}. Stopping monitoring.\")\n",
    "        break\n",
    "\n",
    "    # Fetch and print logs\n",
    "    try:\n",
    "        streams_response = logs_client.describe_log_streams(\n",
    "            logGroupName=log_group_name,\n",
    "            orderBy='LastEventTime',\n",
    "            descending=True,\n",
    "            limit=5  # Check a few most recently active streams\n",
    "        )\n",
    "\n",
    "        active_streams_found_this_poll = False\n",
    "        for stream in streams_response.get('logStreams', []):\n",
    "            active_streams_found_this_poll = True\n",
    "            stream_name = stream['logStreamName']\n",
    "            \n",
    "            start_time_ms = last_event_timestamps.get(stream_name, 0) + 1\n",
    "            \n",
    "            next_token_for_stream_poll = None\n",
    "            stream_had_new_events_this_poll = False\n",
    "\n",
    "            # Paginate through events for this stream in this poll interval\n",
    "            while True:\n",
    "                event_fetch_args = {\n",
    "                    'logGroupName': log_group_name,\n",
    "                    'logStreamName': stream_name,\n",
    "                    'startTime': start_time_ms,\n",
    "                    'limit': 100, # Fetch up to 100 events per call\n",
    "                    'startFromHead': True\n",
    "                }\n",
    "                if next_token_for_stream_poll:\n",
    "                    event_fetch_args['nextToken'] = next_token_for_stream_poll\n",
    "\n",
    "                try:\n",
    "                    events_response = logs_client.get_log_events(**event_fetch_args)\n",
    "                except logs_client.exceptions.ResourceNotFoundException:\n",
    "                    # Stream might have just been created and not yet available for get_log_events\n",
    "                    # Or a transient issue.\n",
    "                    print(f\"    Log stream {stream_name} not found during get_log_events. Will retry.\")\n",
    "                    break \n",
    "                except Exception as e_get:\n",
    "                    print(f\"    Error getting events for stream {stream_name}: {e_get}\")\n",
    "                    break \n",
    "\n",
    "                fetched_events_batch = events_response.get('events', [])\n",
    "                \n",
    "                if fetched_events_batch:\n",
    "                    if not stream_had_new_events_this_poll:\n",
    "                        print(f\"  Logs from stream: {stream_name}\")\n",
    "                        stream_had_new_events_this_poll = True\n",
    "\n",
    "                    for event in fetched_events_batch:\n",
    "                        event_ts_ms = event['timestamp']\n",
    "                        # Only print if newer than last seen for this stream\n",
    "                        if event_ts_ms >= start_time_ms:\n",
    "                             event_time_str = time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime(event_ts_ms / 1000))\n",
    "                             print(f\"    {event_time_str} UTC: {event['message'].strip()}\")\n",
    "                    \n",
    "                    last_event_timestamps[stream_name] = fetched_events_batch[-1]['timestamp']\n",
    "                \n",
    "                next_token_for_stream_poll = events_response.get('nextForwardToken')\n",
    "                if not next_token_for_stream_poll or not fetched_events_batch:\n",
    "                    break\n",
    "        \n",
    "        if not active_streams_found_this_poll and status == \"Creating\":\n",
    "             print(f\"  No active log streams found yet for {log_group_name}.\")\n",
    "\n",
    "    except logs_client.exceptions.ResourceNotFoundException:\n",
    "        print(f\"  Log group {log_group_name} not found yet. This is normal during initial endpoint creation.\")\n",
    "    except Exception as e_logs:\n",
    "        print(f\"  An error occurred while fetching logs: {str(e_logs)}\")\n",
    "\n",
    "    time.sleep(poll_interval_seconds)\n",
    "\n",
    "print(\"\\n--- Monitoring finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3284008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-east-1:992382637587:endpoint/triton-nlp-llama-pt-2025-06-11-12-50-50\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "    print(resp)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082ecd3a",
   "metadata": {},
   "source": [
    "### PyTorch: Run inference\n",
    "\n",
    "Once we have the endpoint running we can use a sample text to do an inference using json as the payload format. For inference request format, Triton uses the KFServing community standard [inference protocols](https://github.com/triton-inference-server/server/blob/main/docs/protocol/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4853965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_payload(text, max_tokens=100):\n",
    "    payload = {}\n",
    "    payload[\"inputs\"] = []\n",
    "    payload[\"inputs\"].append(\n",
    "        {\n",
    "            \"name\": \"INPUT_TEXT\",\n",
    "            \"shape\": [1, 1],\n",
    "            \"datatype\": \"BYTES\",\n",
    "            \"data\": [text],\n",
    "        }\n",
    "    )\n",
    "    payload[\"inputs\"].append(\n",
    "        {\n",
    "            \"name\": \"MAX_TOKENS\",\n",
    "            \"shape\": [1, 1],\n",
    "            \"datatype\": \"INT32\",\n",
    "            \"data\": [[max_tokens]],\n",
    "        }\n",
    "    )\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d703109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'llama', 'model_version': '1', 'outputs': [{'name': 'OUTPUT_TEXT', 'datatype': 'BYTES', 'shape': [1, 1], 'data': ['It delivers high-end inference capabilities on-demand. The Inference Engine (Triton-IE) is a general-purpose, open-source framework that offers various features for developers to accelerate AI applications. The Core Library (Triton-Core) provides a set of machine learning primitives implemented in C/C++ to accelerate inference processing in the cloud or edge. The Web UI is a graphical interface for deploying and running inference tasks.\\nTriton Inference Platform is the underlying framework to build AI applications. It']}]}\n"
     ]
    }
   ],
   "source": [
    "text_triton = [\"Triton Inference Server provides a cloud and edge inferencing solution optimized for both CPUs and GPUs.\"]\n",
    "\n",
    "payload = get_text_payload(text_triton, 100)\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"application/octet-stream\", Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "print(json.loads(response[\"Body\"].read().decode(\"utf8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddb0c0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'llama', 'model_version': '1', 'outputs': [{'name': 'OUTPUT_TEXT', 'datatype': 'BYTES', 'shape': [1, 1], 'data': ['We apply a suite of quality checking methods, and we ensure the quality of the result. Once the quality is verified, your LLM can be deployed.\\nWe are currently offering the highest quality service in the industry. We are looking for potential partners to develop additional models and improve the quality.\\nFor questions or further information, please contact us at contact@PrunAI.org\\nIn case the model does not have enough capacity, we will train the model further with higher quality questions. The questions may come']}]}\n"
     ]
    }
   ],
   "source": [
    "text_triton = [\"Pruna AI offers the best compresion methods for LLMs.\"]\n",
    "\n",
    "payload = get_text_payload(text_triton, 100)\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, ContentType=\"application/octet-stream\", Body=json.dumps(payload)\n",
    ")\n",
    "\n",
    "print(json.loads(response[\"Body\"].read().decode(\"utf8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d14ab1",
   "metadata": {},
   "source": [
    "### PyTorch: Terminate endpoint and clean up artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9a24ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'd7a79939-1178-42a8-ba9a-0e4e44970332',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'd7a79939-1178-42a8-ba9a-0e4e44970332',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Wed, 11 Jun 2025 13:19:05 GMT',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.delete_model(ModelName=sm_model_name)\n",
    "sm.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
